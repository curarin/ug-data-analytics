{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variablen anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Domain soll analysiert werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#Variables \n",
    "### Zunächst wird danach gefiltert, welche Suchbegriffe nicht enthalten sein sollen (notContains)\n",
    "brand = \"guru\"\n",
    "\n",
    "#AT(aut), Switzerland(che), Netherlands(nl), Spain(esp), Germany(deu)\n",
    "COUNTRY_FILTER = [\"esp\"]\n",
    "\n",
    "#DE(urlaubsguru.de), NLD(holidayguru.nl), ES(holidayguru.es), AT(urlaubsguru.at). CH(holidayguru.ch)\n",
    "domain_name = \"holidayguru.es\" \n",
    "\n",
    "#Welche URLs möchtest du dir ansehen & wann wurden diese optimiert?\n",
    "url_list = {\n",
    "    \"https://www.holidayguru.es/calendario-de-viajes/nochevieja/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-en-madrid/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-en-valencia/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-palma/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-malaga/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-en-sevilla/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-murcia/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/fin-de-ano-en-barcelona/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-bilbao/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-en-zaragoza/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/fin-ano-andorra/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-en-londres/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-roma/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/nochevieja-en-paris/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/fin-de-ano-en-praga-consejos-y-trucos/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/tradiciones-de-navidad-alrededor-del-mundo/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/mercadillos-navidenos-en-madrid/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/las-10-mejores-fiestas-populares-de-espaa/\": \"2023-11-29\",\n",
    "    \"https://www.holidayguru.es/revista-de-viajes/paises-diferentes-culturas-diversas/\": \"2023-11-29\"\n",
    "}\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline GSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Bereich werden die Daten direkt aus der GSC gezogen. Dabei gibt es folgende zwei Zeiträume:\n",
    "- Datum der Optimierung (siehe oben) bis heute\n",
    "- Der exakt selbe Zeitraum vor der Optimierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variablen für den API Request\n",
    "DIMENSIONS_BYURL = [\"page\", \"query\", \"date\", \"country\"]\n",
    "DOMAIN = \"sc-domain:\" + domain_name\n",
    "credential_filepath = \".secrets/creds.json\"\n",
    "MAX_ROWS = 25_000\n",
    "\n",
    "#calculate date\n",
    "today = date.today()\n",
    "end_date = today\n",
    "\n",
    "#Functions\n",
    "#Für Timeframe Comparison:\n",
    "\n",
    "def determine_timerange(row, comparison_start_date):\n",
    "    if row[\"date\"] > start_date:\n",
    "        return \"after\"\n",
    "    else:\n",
    "        return \"prior\"\n",
    "\n",
    "## für Auth\n",
    "def auth_using_key_file(key_filepath):\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        key_filepath, scopes=SCOPE\n",
    "    )\n",
    "    service = build(API_SERVICE_NAME, API_VERSION, credentials=credentials)\n",
    "    return service\n",
    "\n",
    "def query(client: Resource, payload: Dict[str, str]) -> Dict[str, any]:\n",
    "    response = client.searchanalytics().query(siteUrl=DOMAIN, body=payload).execute()\n",
    "    return response\n",
    "\n",
    "# Plot Function (wird mit Input aus der Loop gefüttert)\n",
    "def plot_data(title, x_label, y_label, data_frame, url):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    data_frame = data_frame[[\"prior\", \"after\"]]\n",
    "    for column in data_frame.columns:\n",
    "        filtered_data = data_frame[data_frame[column] != 0]\n",
    "        ax.plot(filtered_data.index, filtered_data[column], marker='o', label=column)\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot Function (wird mit Input aus der Loop gefüttert)\n",
    "def plot_data_inverted(title, x_label, y_label, data_frame, url):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    data_frame = data_frame[[\"prior\", \"after\"]]\n",
    "    for column in data_frame.columns:\n",
    "        filtered_data = data_frame[data_frame[column] != 0]\n",
    "        ax.plot(filtered_data.index, filtered_data[column], marker='o', label=column)\n",
    "\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#metric difference calculation\n",
    "def calculate_difference(df, url, query):\n",
    "    prior_clicks = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"prior\") & (df[\"query\"] == query)][\"clicks\"].sum()\n",
    "    after_clicks = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"after\") & (df[\"query\"] == query)][\"clicks\"].sum()\n",
    "\n",
    "    prior_avg_position = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"prior\") & (df[\"query\"] == query)][\"position\"].mean()\n",
    "    after_avg_position = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"after\") & (df[\"query\"] == query)][\"position\"].mean()\n",
    "\n",
    "    clicks_difference = after_clicks - prior_clicks\n",
    "    avg_position_difference = after_avg_position - prior_avg_position\n",
    "\n",
    "    return clicks_difference, avg_position_difference, prior_clicks, after_clicks, prior_avg_position, after_avg_position\n",
    "\n",
    "############################################################################################\n",
    "# GSC API\n",
    "API_SERVICE_NAME = \"webmasters\"\n",
    "API_VERSION = \"v3\"\n",
    "SCOPE = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "\n",
    "KEY_FILE = credential_filepath\n",
    "service = auth_using_key_file(key_filepath=KEY_FILE)\n",
    "############################################################################################\n",
    "#Beginn des API Requests\n",
    "# dataframe um daten über die API zu speichern\n",
    "data_frames = []\n",
    "\n",
    "# by URL\n",
    "for url, date in url_list.items():\n",
    "    #start date calculatin\n",
    "    start_date = datetime.datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "    #comparison date range\n",
    "    delta_days = (start_date - end_date).days\n",
    "    delta_days *= -1\n",
    "    comparison_start_date = start_date - timedelta(days=delta_days)\n",
    "    comparison_end_date = end_date - timedelta(days=delta_days)\n",
    "    start_date = start_date + timedelta(days=1)\n",
    "    \n",
    "    i = 0\n",
    "    reponse_by_url = []\n",
    "    while True:\n",
    "        payload_main_range = {\n",
    "            \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"dimensions\": DIMENSIONS_BYURL,\n",
    "            \"dimensionFilterGroups\": [{\n",
    "                \"filters\": [{\n",
    "                    \"dimension\": \"page\",\n",
    "                    \"expression\": url\n",
    "                },\n",
    "                    {\n",
    "                    \"dimension\": \"country\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"expression\": COUNTRY_FILTER\n",
    "                    }]\n",
    "            }],\n",
    "            \"rowLimit\": MAX_ROWS,\n",
    "            \"startRow\": i * MAX_ROWS\n",
    "        }\n",
    "\n",
    "        # make request to API\n",
    "        response_main_range = query(service, payload_main_range)\n",
    "\n",
    "        # if there are rows in the response, append to the temporary list\n",
    "        if response_main_range.get(\"rows\"):\n",
    "            reponse_by_url.extend(response_main_range[\"rows\"])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        print(f\"Collected {len(reponse_by_url):,} rows (main range) for {url}.\")\n",
    "    \n",
    "    # Create a DataFrame from the temporary list\n",
    "    by_url_data = pd.DataFrame(reponse_by_url)\n",
    "    by_url_data[DIMENSIONS_BYURL] = pd.DataFrame(by_url_data[\"keys\"].tolist(), index=by_url_data.index)\n",
    "    by_url_data = by_url_data.drop(columns=\"keys\")\n",
    "\n",
    "    # Add a new column \"timerange\" and populate it based on payload range\n",
    "    by_url_data[\"date\"] = pd.to_datetime(by_url_data[\"date\"]).dt.date\n",
    "    by_url_data[\"timerange\"] = by_url_data.apply(lambda row: determine_timerange(row, comparison_start_date), axis=1)\n",
    "    data_frames.append(by_url_data)\n",
    "\n",
    "    a = 0\n",
    "    response_by_comparison_url = []\n",
    "    while True:\n",
    "        payload_comparison_range = {\n",
    "            \"startDate\": comparison_start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"endDate\": comparison_end_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"dimensions\": DIMENSIONS_BYURL,\n",
    "             \"dimensionFilterGroups\": [{\n",
    "                \"filters\": [{\n",
    "                    \"dimension\": \"page\",\n",
    "                    \"expression\": url\n",
    "                },\n",
    "                    {\n",
    "                    \"dimension\": \"country\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"expression\": COUNTRY_FILTER\n",
    "                    }]\n",
    "            }],\n",
    "            \"rowLimit\": MAX_ROWS,\n",
    "            \"startRow\": a * MAX_ROWS\n",
    "        }\n",
    "\n",
    "        # make request to API\n",
    "        response_comparison_range = query(service, payload_comparison_range)\n",
    "\n",
    "        # if there are rows in the response, append to the temporary list\n",
    "        if response_comparison_range.get(\"rows\"):\n",
    "            response_by_comparison_url.extend(response_comparison_range[\"rows\"])\n",
    "            a += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        print(f\"Collected {len(reponse_by_url):,} rows (comparison range) for {url}.\")\n",
    "\n",
    "    # Create a DataFrame from the temporary list\n",
    "    by_url_data_response = pd.DataFrame(response_by_comparison_url)\n",
    "    by_url_data_response[DIMENSIONS_BYURL] = pd.DataFrame(by_url_data_response[\"keys\"].tolist(), index=by_url_data_response.index)\n",
    "    by_url_data_response = by_url_data_response.drop(columns=\"keys\")\n",
    "\n",
    "    # Add a new column \"timerange\" and populate it based on payload range\n",
    "    by_url_data_response[\"date\"] = pd.to_datetime(by_url_data_response[\"date\"]).dt.date\n",
    "    by_url_data_response[\"timerange\"] = by_url_data_response.apply(lambda row: determine_timerange(row, comparison_start_date), axis=1)\n",
    "\n",
    "    # Append the data frame to the list\n",
    "    data_frames.append(by_url_data_response)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "all_data = pd.concat(data_frames, ignore_index=True)\n",
    "df = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufbereitung der Daten mit dem Ziel:\n",
    "- Zeiträume vor und nach der Optimierung zu aggregieren\n",
    "- Ausspielen der Tabellen & Plots pro URL\n",
    "\n",
    "Am Ende folgt die Erkenntnis:\n",
    "- Das war das Top Keyword VOR der Optimierung\n",
    "- Das war das Top Keyword NACH der Optimierung\n",
    "- So haben sich Clicks, Query Count, Average Ranking des Top Keywords (nach Klicks) verändert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulation\n",
    "df = df[~df[\"query\"].str.contains(brand)] # Brand Queries raushauen#\n",
    "# Convert \"date\" column to datetime format\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# Create a new column \"time_group\" based on the year and week\n",
    "#df['time_group'] = df['date'].dt.strftime('%Y-%U')\n",
    "df['time_group'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "# Assign a time label\n",
    "time_label = \"Year-Week\"\n",
    "\n",
    "for url, date in url_list.items():\n",
    "    filtered_df = df[df[\"page\"] == url]\n",
    "    title_overall = f\"## Performance Change für {url}\\n\\n\"\n",
    "    display(Markdown(title_overall))\n",
    "    \n",
    "    #click daten\n",
    "    filtered_df_pivot_clicks = pd.pivot_table(\n",
    "        filtered_df, \n",
    "        values=\"clicks\", \n",
    "        index=[\"time_group\"], \n",
    "        columns=[\"timerange\"], \n",
    "        aggfunc=np.sum).fillna(0).astype(int)\n",
    "\n",
    "    #query calculations\n",
    "    diff_data = []\n",
    "    top_queries_prior = filtered_df[filtered_df[\"timerange\"] == \"prior\"].groupby(\"query\")[\"clicks\"].sum().nlargest(10)\n",
    "    for query in top_queries_prior.index:\n",
    "        clicks_diff, avg_position_diff, prior_clicks, after_clicks, prior_avg_position, after_avg_position = calculate_difference(filtered_df, url, query)\n",
    "        diff_data.append({\"Query\": query, \"Clicks Prior\": prior_clicks, \"Clicks After\": after_clicks, \"Clicks Difference\": clicks_diff, \"Position Prior\": prior_avg_position, \"Position After\": after_avg_position, \"Avg Position Difference\": avg_position_diff})\n",
    "    difference_df = pd.DataFrame(diff_data)\n",
    "    difference_df[\"Avg Position Difference\"] = difference_df[\"Avg Position Difference\"].apply(lambda x: round(x, 2))\n",
    "    difference_df[\"Position Prior\"] = difference_df[\"Position Prior\"].apply(lambda x: round(x, 2))\n",
    "    difference_df[\"Position After\"] = difference_df[\"Position After\"].apply(lambda x: round(x, 2))\n",
    "    title_markdown = \"### Suchbegriffe, welche vor der Optimierung die meisten Klicks brachten, haben sich wie folgt verändert.\\n\\n\"\n",
    "    display(Markdown(title_markdown))\n",
    "    display(difference_df)\n",
    "    \n",
    "    diff_data = []\n",
    "    top_queries_after = filtered_df[filtered_df[\"timerange\"] == \"after\"].groupby(\"query\")[\"clicks\"].sum().nlargest(10)\n",
    "    for query in top_queries_after.index:\n",
    "        clicks_diff, avg_position_diff, prior_clicks, after_clicks, prior_avg_position, after_avg_position = calculate_difference(filtered_df, url, query)\n",
    "        diff_data.append({\"Query\": query, \"Clicks Prior\": prior_clicks, \"Clicks After\": after_clicks, \"Clicks Difference\": clicks_diff, \"Position Prior\": prior_avg_position, \"Position After\": after_avg_position, \"Avg Position Difference\": avg_position_diff})\n",
    "    difference_df = pd.DataFrame(diff_data)\n",
    "    difference_df[\"Avg Position Difference\"] = difference_df[\"Avg Position Difference\"].apply(lambda x: round(x, 2))\n",
    "    difference_df[\"Position Prior\"] = difference_df[\"Position Prior\"].apply(lambda x: round(x, 2))\n",
    "    difference_df[\"Position After\"] = difference_df[\"Position After\"].apply(lambda x: round(x, 2))\n",
    "    title_markdown = \"### Diese Suchbegriffe bringen nach der Optimierung die meisten Klicks.\\n\\n\"\n",
    "    display(Markdown(title_markdown))\n",
    "    display(difference_df)  \n",
    "    \n",
    "    # Pivot table for query count over time\n",
    "    filtered_df_forquerycount = filtered_df[filtered_df[\"clicks\"] > 0]\n",
    "    filtered_df_pivot_querycount = pd.pivot_table(\n",
    "        filtered_df_forquerycount,\n",
    "        values=\"query\",\n",
    "        index=[\"time_group\"],\n",
    "        columns=[\"timerange\"],\n",
    "        aggfunc=\"count\"\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    #top keyword\n",
    "    top_kw = filtered_df.groupby(by=\"query\")[\"clicks\"].sum().sort_values(ascending=False).head(1).index.tolist()\n",
    "    top_kw_as_string = \" \".join(top_kw)\n",
    "    filtered_df_for_query = filtered_df[filtered_df[\"query\"].isin(top_kw)]\n",
    "    filtered_df_pivot_position = pd.pivot_table(\n",
    "        filtered_df_for_query, \n",
    "        values=\"position\", \n",
    "        index=[\"time_group\"], \n",
    "        columns=[\"timerange\"], \n",
    "        aggfunc=np.average).fillna(0).astype(float)\n",
    "    \n",
    "    #plotting\n",
    "    plot_click_title = \"### Visualisierung des Klick-Verlaufs\"\n",
    "    display(Markdown(plot_click_title))\n",
    "    plot_data(\n",
    "         title=f\"{DOMAIN}: Clicks for \\\"{url}\\\"\",\n",
    "         x_label=\"Date\",\n",
    "         y_label=\"Sum of Clicks\",\n",
    "         data_frame=filtered_df_pivot_clicks,\n",
    "         url=url\n",
    "    )\n",
    "    plot_qcount_title = \"### Visualisierung des Query-Count-Verlaufs\"\n",
    "    display(Markdown(plot_qcount_title))\n",
    "\n",
    "    plot_data(\n",
    "        title=f\"{DOMAIN}: Unique Query Count for URL \\\"{url}\\\"\",\n",
    "        x_label=\"Date\",\n",
    "        y_label=\"Unique Query Count (Clicks > 0)\",\n",
    "        data_frame=filtered_df_pivot_querycount,\n",
    "        url=url\n",
    "    )\n",
    "    plot_avgposition_title = \"### Visualisierung des Top-Keyword-Rankings\"\n",
    "    display(Markdown(plot_avgposition_title))\n",
    "    plot_data_inverted(\n",
    "        title=f\"Average Ranking for \\\"{top_kw_as_string}\\\" | URL: \\\"{url}\\\"\",\n",
    "        x_label=\"Date\",\n",
    "        y_label=\"Average Ranking\",\n",
    "        data_frame=filtered_df_pivot_position,\n",
    "        url=url\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
