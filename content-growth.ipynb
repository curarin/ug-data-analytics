{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.stats import linregress\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variablen anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Domain soll analysiert werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#Variables \n",
    "### Zunächst wird danach gefiltert, welche Suchbegriffe nicht enthalten sein sollen (notContains)\n",
    "brand = \"guru\"\n",
    "\n",
    "#AT(aut), Switzerland(che), Netherlands(nl), Spain(esp), Germany(deu)\n",
    "COUNTRY_FILTER = [\"deu\"]\n",
    "\n",
    "#DE(urlaubsguru.de), NLD(holidayguru.nl), ES(holidayguru.es), AT(urlaubsguru.at). CH(holidayguru.ch)\n",
    "domain_name = \"urlaubsguru.de\" \n",
    "\n",
    "#wie viele tage an daten möchtest du haben?\n",
    "delta_days = 30\n",
    "\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS_BYURL = [\"page\", \"query\", \"date\", \"country\"]\n",
    "DOMAIN = \"sc-domain:\" + domain_name\n",
    "credential_filepath = \".secrets/creds.json\"\n",
    "\n",
    "#calculate date\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=delta_days)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline GSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Bereich werden die Daten der letzten 30 Tage aus der Google Search Console gezogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#Functions\n",
    "#Für Timeframe Comparison:\n",
    "\n",
    "def determine_timerange(row, comparison_start_date):\n",
    "    if row[\"date\"] > start_date:\n",
    "        return \"after\"\n",
    "    else:\n",
    "        return \"prior\"\n",
    "\n",
    "## für Auth\n",
    "def auth_using_key_file(key_filepath):\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        key_filepath, scopes=SCOPE\n",
    "    )\n",
    "    service = build(API_SERVICE_NAME, API_VERSION, credentials=credentials)\n",
    "    return service\n",
    "\n",
    "def query(client: Resource, payload: Dict[str, str]) -> Dict[str, any]:\n",
    "    response = client.searchanalytics().query(siteUrl=DOMAIN, body=payload).execute()\n",
    "    return response\n",
    "\n",
    "# Plot Function (wird mit Input aus der Loop gefüttert)\n",
    "def plot_data(title, x_label, y_label, data_frame, url):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for column in data_frame.columns:\n",
    "        #filter out values containing 0\n",
    "        filtered_data = data_frame[data_frame[column] != 0]\n",
    "        ax.plot(filtered_data.index, filtered_data[column], marker='o', label=column)\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_inverted_y_axis(title, x_label, y_label, data_frame, url):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for column in data_frame.columns:\n",
    "        # Filter out values containing 0\n",
    "        filtered_data = data_frame[data_frame[column] != 0]\n",
    "        ax.plot(filtered_data.index, filtered_data[column], marker='o', label=column)\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Invert the y-axis\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot Trend Line with Clicks\n",
    "def add_trend_line(ax, data_frame, x_label, y_label, title):\n",
    "    x = np.arange(len(data_frame))\n",
    "    y = data_frame.sum(axis=1)  # Sum the values across columns to get a 1D array\n",
    "\n",
    "    # Fit a linear regression model to your data\n",
    "    coeffs = np.polyfit(x, y, 1)\n",
    "    trend_line = np.poly1d(coeffs)\n",
    "\n",
    "    x_values = np.arange(len(data_frame))  # Create a numeric array for x values\n",
    "\n",
    "    for column in data_frame.columns:\n",
    "        ax.plot(x_values, data_frame[column], marker='o', label=column)\n",
    "\n",
    "    ax.plot(x_values, trend_line(x_values), linestyle=\"--\", label=\"Trend Line\")\n",
    "    \n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#metric difference calculation\n",
    "def calculate_difference(df, url, query):\n",
    "    prior_clicks = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"prior\") & (df[\"query\"] == query)][\"clicks\"].sum()\n",
    "    after_clicks = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"after\") & (df[\"query\"] == query)][\"clicks\"].sum()\n",
    "\n",
    "    prior_avg_position = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"prior\") & (df[\"query\"] == query)][\"position\"].mean()\n",
    "    after_avg_position = df[(df[\"page\"] == url) & (df[\"timerange\"] == \"after\") & (df[\"query\"] == query)][\"position\"].mean()\n",
    "\n",
    "    clicks_difference = after_clicks - prior_clicks\n",
    "    avg_position_difference = after_avg_position - prior_avg_position\n",
    "\n",
    "    return clicks_difference, avg_position_difference, prior_clicks, after_clicks, prior_avg_position, after_avg_position\n",
    "\n",
    "\n",
    "def plot_dual_bar_chart(data_df, x_label, y1_label, y2_label, y1_color='skyblue', y2_color='salmon', rotation=45):\n",
    "    x = np.arange(len(data_df.index))\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    bars1 = ax1.bar(x - bar_width/2, data_df[y1_label], width=bar_width, color=y1_color, label=y1_label)\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y1_label, color=y1_color)\n",
    "    ax1.tick_params(axis='y', labelcolor=y1_color)\n",
    "\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.2f}', \n",
    "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3),  # 3 points vertical offset\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    bars2 = ax2.bar(x + bar_width/2, data_df[y2_label], width=bar_width, color=y2_color, label=y2_label)\n",
    "    ax2.set_ylabel(y2_label, color=y2_color)\n",
    "    ax2.tick_params(axis='y', labelcolor=y2_color)\n",
    "\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.annotate(f'{int(height)}', \n",
    "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3),  # 3 points vertical offset\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(data_df.index, rotation=rotation)\n",
    "\n",
    "    plt.title(f'{y1_label} and {y2_label} by {x_label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# GSC API\n",
    "API_SERVICE_NAME = \"webmasters\"\n",
    "API_VERSION = \"v3\"\n",
    "SCOPE = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "MAX_ROWS = 25_000\n",
    "\n",
    "KEY_FILE = credential_filepath\n",
    "service = auth_using_key_file(key_filepath=KEY_FILE)\n",
    "\n",
    "# by URL\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "reponse_by_url = []\n",
    "while True:\n",
    "    payload_main_range = {\n",
    "        \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"dimensions\": DIMENSIONS_BYURL,\n",
    "        \"dimensionFilterGroups\": [{\n",
    "            \"filters\": [{\n",
    "                \"dimension\": \"country\",\n",
    "                \"operator\": \"contains\",\n",
    "                \"expression\": COUNTRY_FILTER\n",
    "                },\n",
    "            {\n",
    "                \"dimension\": \"query\",\n",
    "                \"operator\": \"notContains\",\n",
    "                \"expression\": brand\n",
    "            },\n",
    "            {\n",
    "                \"dimension\": \"page\",\n",
    "                \"operator\": \"notContains\",\n",
    "                \"expression\": \"#\"\n",
    "            }]\n",
    "        }],\n",
    "        \"rowLimit\": MAX_ROWS,\n",
    "        \"startRow\": i * MAX_ROWS\n",
    "    }\n",
    "\n",
    "    # make request to API\n",
    "    response_main_range = query(service, payload_main_range)\n",
    "\n",
    "    # if there are rows in the response, append to the temporary list\n",
    "    if response_main_range.get(\"rows\"):\n",
    "        reponse_by_url.extend(response_main_range[\"rows\"])\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    print(f\"Collected {len(reponse_by_url):,} rows.\")\n",
    "    \n",
    "# Create a DataFrame from the temporary list\n",
    "by_url_data = pd.DataFrame(reponse_by_url)\n",
    "by_url_data[DIMENSIONS_BYURL] = pd.DataFrame(by_url_data[\"keys\"].tolist(), index=by_url_data.index)\n",
    "df = by_url_data.drop(columns=\"keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datentypen anpassen\n",
    "df[\"position\"] = df[\"position\"].round(3)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "df[\"Month\"] = df[\"date\"].dt.month\n",
    "df[\"Day\"] = df[\"date\"].dt.day\n",
    "df[\"Year\"] = df[\"date\"].dt.year\n",
    "df[\"Day of Year\"] = df[\"date\"].dt.dayofyear\n",
    "df[\"Year Month\"] = df[\"date\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "### Daten pivotieren\n",
    "table = pd.pivot_table(df, values=\"clicks\", index=[\"page\"], columns=[\"Day of Year\"], aggfunc=np.sum)\n",
    "table = table.fillna(0)\n",
    "table = table.astype(int)\n",
    "table = table.reset_index()\n",
    "#table[\"Highest Amount of Clicks\"] = table.loc[:, table.columns.str.match(\"20[0-9]{2}-[0-9]{2}\")].max(axis=1)\n",
    "\n",
    "table[\"Sum of Clicks\"] = table.loc[:, [col for col in table.columns if str(col).isdigit() and 1 <= int(col) <= 400]].sum(axis=1)\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "for row in table.itertuples(index=False):\n",
    "    values_list = row[2:-1]\n",
    "    x_values = [i + 1 for i in range(len(values_list))]\n",
    "    \n",
    "    if len(set(values_list)) > 1:\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(values_list, x_values)\n",
    "        rows_list.append(slope)\n",
    "    else:\n",
    "        rows_list.append(None)  # or any other default value, as linear regression is not possible\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df = pd.DataFrame(rows_list, columns=[\"Slope\"])\n",
    "slope_df[\"Slope\"] = (slope_df[\"Slope\"] * 10).round(3)\n",
    "# Dataframes Join on Index\n",
    "df_w_slope = pd.merge(table, slope_df, left_index=True, right_index=True)\n",
    "\n",
    "columns_to_keep = [col for col in df_w_slope.columns if str(col).isdigit() and 1 <= int(col) <= 400]\n",
    "\n",
    "# Select the columns to keep\n",
    "df_w_slope = df_w_slope.drop(columns=columns_to_keep)\n",
    "\n",
    "# Binning\n",
    "num_bins = 10\n",
    "percentiles = [i * 100 / num_bins for i in range(num_bins + 1)]\n",
    "bin_edges = [df_w_slope[\"Sum of Clicks\"].quantile(p / 100) for p in percentiles]\n",
    "\n",
    "# Create labels for the bins\n",
    "#bin_labels = [f'Bin {i+1}' for i in range(num_bins)]\n",
    "\n",
    "# Create a new column with the binned data\n",
    "df_w_slope[\"Clicks Binned\"] = pd.cut(df_w_slope[\"Sum of Clicks\"], bins=bin_edges, labels=False, duplicates=\"drop\", include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bins Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "############################################################################################\n",
    "### VIZUALIZE BINS AND STUFF\n",
    "\n",
    "# Pivot the DataFrame to get average slope and unique page counts for each bin\n",
    "df_w_slope_positive = df_w_slope[df_w_slope[\"Slope\"] > 0]\n",
    "df_w_slope_negative = df_w_slope[df_w_slope[\"Slope\"] < 0]\n",
    "pivot_df_negative = df_w_slope_negative.pivot_table(index='Clicks Binned', values=['Slope', 'Sum of Clicks'], aggfunc={'Slope': 'mean', 'Sum of Clicks': 'sum'})\n",
    "pivot_df_positive = df_w_slope_positive.pivot_table(index='Clicks Binned', values=['Slope', 'Sum of Clicks'], aggfunc={'Slope': 'mean', 'Sum of Clicks': 'sum'})\n",
    "\n",
    "#Plotting\n",
    "display(Markdown(f\"## Negative Slope for last {delta_days} Days\\n\\n\"))\n",
    "plot_dual_bar_chart(pivot_df_negative, 'Bins', 'Slope', 'Sum of Clicks')\n",
    "display(Markdown(f\"## Positive Slope for last {delta_days} Days\\n\\n\"))\n",
    "plot_dual_bar_chart(pivot_df_positive, 'Bins', 'Slope', 'Sum of Clicks')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# CHOOSE BINS TO ANALYZE\n",
    "bins_category_wanted = 9\n",
    "positive_or_negative = \"negative\" #either \"positive\" or \"negative\"\n",
    "top_n_results_wanted = 100\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if positive_or_negative == \"negative\":\n",
    "    df_w_slope_negative = df_w_slope[(df_w_slope[\"Slope\"] < 0) & (df_w_slope[\"Clicks Binned\"] == bins_category_wanted)]\n",
    "    top_n_negative_slope = df_w_slope_negative.sort_values(by=[\"Sum of Clicks\", \"Slope\"], ascending=[False, True]).head(top_n_results_wanted)\n",
    "    slope_list = top_n_negative_slope[\"page\"].tolist()\n",
    "else:\n",
    "    df_w_slope_positive = df_w_slope[(df_w_slope[\"Slope\"] > 0) & (df_w_slope[\"Clicks Binned\"] == bins_category_wanted)]\n",
    "    top_n_positive_slope = df_w_slope_positive.sort_values(by=[\"Sum of Clicks\", \"Slope\"], ascending=[False, False]).head(top_n_results_wanted)\n",
    "    slope_list = top_n_positive_slope[\"page\"].tolist()\n",
    "\n",
    "#calculate date and stuff\n",
    "today = date.today()\n",
    "end_date = today\n",
    "start_date = end_date - timedelta(days=delta_days)\n",
    "end_date_comp = start_date - timedelta(days=1)\n",
    "start_date_comp = end_date_comp - timedelta(days=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline GSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird nun für jede URL in dem jeweiligen Bin aktuelle Daten sowie Vergleichsdaten des Zeitraums zuvor gezogen. Dies dient dazu etwas \"Kontext\" zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# GSC API FOR CHANGE STUFF\n",
    "API_SERVICE_NAME = \"webmasters\"\n",
    "API_VERSION = \"v3\"\n",
    "SCOPE = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "MAX_ROWS = 25_000\n",
    "\n",
    "KEY_FILE = credential_filepath\n",
    "service = auth_using_key_file(key_filepath=KEY_FILE)\n",
    "\n",
    "\n",
    "def query(client: Resource, payload: Dict[str, str]) -> Dict[str, any]:\n",
    "    response = client.searchanalytics().query(siteUrl=DOMAIN, body=payload).execute()\n",
    "    return response\n",
    "\n",
    "# by URL\n",
    "#start date calculatin\n",
    "start_date = end_date - timedelta(days=delta_days)\n",
    "\n",
    "# dataframe um daten über die API zu speichern\n",
    "data_frames = []\n",
    "\n",
    "c = 0 #counter for print\n",
    "# by URL\n",
    "for url in slope_list:\n",
    "    i = 0\n",
    "    reponse_by_url = []\n",
    "    while True:\n",
    "        payload_main_range = {\n",
    "            \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"dimensions\": DIMENSIONS_BYURL,\n",
    "            \"dimensionFilterGroups\": [{\n",
    "                \"filters\": [{\n",
    "                    \"dimension\": \"page\",\n",
    "                    \"expression\": url\n",
    "                },\n",
    "                    {\n",
    "                    \"dimension\": \"country\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"expression\": COUNTRY_FILTER\n",
    "                    }]\n",
    "            }],\n",
    "            \"rowLimit\": MAX_ROWS,\n",
    "            \"startRow\": i * MAX_ROWS\n",
    "        }\n",
    "\n",
    "        # make request to API\n",
    "        response_main_range = query(service, payload_main_range)\n",
    "\n",
    "        # if there are rows in the response, append to the temporary list\n",
    "        if response_main_range.get(\"rows\"):\n",
    "            reponse_by_url.extend(response_main_range[\"rows\"])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        #print(f\"Collected {len(reponse_by_url):,} rows (main range) for {url}.\")\n",
    "    \n",
    "    # Create a DataFrame from the temporary list\n",
    "    by_url_data = pd.DataFrame(reponse_by_url)\n",
    "    if \"keys\" in by_url_data.columns:\n",
    "        by_url_data[DIMENSIONS_BYURL] = pd.DataFrame(by_url_data[\"keys\"].tolist(), index=by_url_data.index)\n",
    "        by_url_data = by_url_data.drop(columns=\"keys\")\n",
    "\n",
    "        # Add a new column \"timerange\" and populate it based on payload range\n",
    "        by_url_data[\"date\"] = pd.to_datetime(by_url_data[\"date\"]).dt.date\n",
    "        by_url_data[\"timerange\"] = by_url_data.apply(lambda row: determine_timerange(row, start_date), axis=1)\n",
    "        data_frames.append(by_url_data)\n",
    "    else:\n",
    "        print(f\"Skipping processing for URL {url} as 'keys' column is not present.\")\n",
    "    \n",
    "    a = 0\n",
    "    response_by_comparison_url = []\n",
    "    while True:\n",
    "        payload_comparison_range = {\n",
    "            \"startDate\": start_date_comp.strftime(\"%Y-%m-%d\"),\n",
    "            \"endDate\": end_date_comp.strftime(\"%Y-%m-%d\"),\n",
    "            \"dimensions\": DIMENSIONS_BYURL,\n",
    "             \"dimensionFilterGroups\": [{\n",
    "                \"filters\": [{\n",
    "                    \"dimension\": \"page\",\n",
    "                    \"expression\": url\n",
    "                },\n",
    "                    {\n",
    "                    \"dimension\": \"country\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"expression\": COUNTRY_FILTER\n",
    "                    }]\n",
    "            }],\n",
    "            \"rowLimit\": MAX_ROWS,\n",
    "            \"startRow\": a * MAX_ROWS\n",
    "        }\n",
    "\n",
    "        # make request to API\n",
    "        response_comparison_range = query(service, payload_comparison_range)\n",
    "\n",
    "        # if there are rows in the response, append to the temporary list\n",
    "        if response_comparison_range.get(\"rows\"):\n",
    "            response_by_comparison_url.extend(response_comparison_range[\"rows\"])\n",
    "            a += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        #print(f\"Collected {len(reponse_by_url):,} rows (comparison range) for {url}.\")\n",
    "        c += 1\n",
    "        print(f\"Collecting data for page {c}/{len(slope_list)}...\")\n",
    "\n",
    "    # Create a DataFrame from the temporary list\n",
    "    by_url_data_response = pd.DataFrame(response_by_comparison_url)\n",
    "    if \"keys\" in by_url_data_response.columns:\n",
    "        by_url_data_response[DIMENSIONS_BYURL] = pd.DataFrame(by_url_data_response[\"keys\"].tolist(), index=by_url_data_response.index)\n",
    "        by_url_data_response = by_url_data_response.drop(columns=\"keys\")\n",
    "\n",
    "        # Add a new column \"timerange\" and populate it based on payload range\n",
    "        by_url_data_response[\"date\"] = pd.to_datetime(by_url_data_response[\"date\"]).dt.date\n",
    "        by_url_data_response[\"timerange\"] = by_url_data_response.apply(lambda row: determine_timerange(row, start_date_comp), axis=1)\n",
    "\n",
    "        # Append the data frame to the list\n",
    "        data_frames.append(by_url_data_response)\n",
    "    else:\n",
    "        print(f\"Skipping processing for URL {url} as 'keys' column is not present.\")\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "all_data = pd.concat(data_frames, ignore_index=True)\n",
    "df = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulation\n",
    "df = df[~df[\"query\"].str.contains(brand)] # Brand Queries raushauen#\n",
    "df = df[~(df == False).all(axis=1)]\n",
    "\n",
    "# Convert \"date\" column to datetime format\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# Create a new column \"time_group\" based on the year and week\n",
    "df['time_group'] = df['date'].dt.strftime('%Y-%U')\n",
    "# Assign a time label\n",
    "time_label = \"Year-Week\"\n",
    "\n",
    "for url in slope_list:\n",
    "    filtered_df = df[df[\"page\"] == url]\n",
    "    title_overall = f\"## Performance Overview für {url}\\n\\n\"\n",
    "    display(Markdown(title_overall))\n",
    "    \n",
    "    #click daten\n",
    "    filtered_df_pivot_clicks = pd.pivot_table(\n",
    "        filtered_df, \n",
    "        values=\"clicks\", \n",
    "        index=[\"date\"], #[\"time_group\"]\n",
    "        columns=[\"timerange\"], \n",
    "        aggfunc=np.sum).fillna(0).astype(int)\n",
    "\n",
    "    #click daten im letzten monat\n",
    "    filtered_df_only_after_timerange = filtered_df[filtered_df[\"timerange\"] == \"after\"]\n",
    "    filtered_df_pivot_clicks_only_after_timerange = pd.pivot_table(\n",
    "        filtered_df_only_after_timerange, \n",
    "        values=\"clicks\", \n",
    "        index=[\"date\"], #[\"time_group\"]\n",
    "        columns=[\"timerange\"], \n",
    "        aggfunc=np.sum).fillna(0).astype(int)\n",
    "\n",
    "    #query calculations\n",
    "    diff_data = []\n",
    "    top_queries_prior = filtered_df[filtered_df[\"timerange\"] == \"prior\"].groupby(\"query\")[\"clicks\"].sum().nlargest(10)\n",
    "    for query in top_queries_prior.index:\n",
    "        clicks_diff, avg_position_diff, prior_clicks, after_clicks, prior_avg_position, after_avg_position = calculate_difference(filtered_df, url, query)\n",
    "        diff_data.append({\"Query\": query, \"Clicks Prior\": prior_clicks, \"Clicks After\": after_clicks, \"Clicks Difference\": clicks_diff, \"Position Prior\": prior_avg_position, \"Position After\": after_avg_position, \"Avg Position Difference\": avg_position_diff})\n",
    "    difference_df = pd.DataFrame(diff_data)\n",
    "    difference_df[\"Avg Position Difference\"] = difference_df[\"Avg Position Difference\"].apply(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    "    difference_df[\"Position Prior\"] = difference_df[\"Position Prior\"].apply(lambda x: round(x, 2))\n",
    "    difference_df[\"Position After\"] = difference_df[\"Position After\"].apply(lambda x: round(x, 2))\n",
    "    title_markdown = \"### Suchbegriffe, welche im Monat zuvor die meisten Klicks brachten, haben sich wie folgt verändert.\\n\\n\"\n",
    "    display(Markdown(title_markdown))\n",
    "    display(difference_df)\n",
    "    \n",
    "    diff_data = []\n",
    "    top_queries_after = filtered_df[filtered_df[\"timerange\"] == \"after\"].groupby(\"query\")[\"clicks\"].sum().nlargest(10)\n",
    "    for query in top_queries_after.index:\n",
    "        clicks_diff, avg_position_diff, prior_clicks, after_clicks, prior_avg_position, after_avg_position = calculate_difference(filtered_df, url, query)\n",
    "        diff_data.append({\"Query\": query, \"Clicks Prior\": prior_clicks, \"Clicks After\": after_clicks, \"Clicks Difference\": clicks_diff, \"Position Prior\": prior_avg_position, \"Position After\": after_avg_position, \"Avg Position Difference\": avg_position_diff})\n",
    "    difference_df = pd.DataFrame(diff_data)\n",
    "    difference_df[\"Avg Position Difference\"] = difference_df[\"Avg Position Difference\"].apply(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    "    difference_df[\"Position Prior\"] = difference_df[\"Position Prior\"].apply(lambda x: round(x, 2))\n",
    "    difference_df[\"Position After\"] = difference_df[\"Position After\"].apply(lambda x: round(x, 2))\n",
    "\n",
    "\n",
    "    \n",
    "    # Pivot table for query count over time\n",
    "    filtered_df_forquerycount = filtered_df[filtered_df[\"clicks\"] > 0]\n",
    "    filtered_df_pivot_querycount = pd.pivot_table(\n",
    "        filtered_df_forquerycount,\n",
    "        values=\"query\",\n",
    "        index=[\"date\"],\n",
    "        columns=[\"timerange\"],\n",
    "        aggfunc=\"count\"\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    #top keyword\n",
    "    top_kw = filtered_df.groupby(by=\"query\")[\"clicks\"].sum().sort_values(ascending=False).head(1).index.tolist()\n",
    "    top_kw_as_string = \" \".join(top_kw)\n",
    "    filtered_df_for_query = filtered_df[filtered_df[\"query\"].isin(top_kw)]\n",
    "    filtered_df_pivot_position = pd.pivot_table(\n",
    "        filtered_df_for_query, \n",
    "        values=\"position\", \n",
    "        index=[\"date\"], \n",
    "        columns=[\"timerange\"], \n",
    "        aggfunc=np.average).fillna(0).astype(float)\n",
    "    \n",
    "    #plotting\n",
    "    plot_click_title_month_over_month = \"### Visualization of clicks over time vs. previous month\"\n",
    "    display(Markdown(plot_click_title_month_over_month))\n",
    "    plot_data(\n",
    "        title=f\"Clicks for \\\"{url}\\\"\",\n",
    "        x_label=\"Date\",\n",
    "        y_label=\"Sum of Clicks\",\n",
    "        data_frame=filtered_df_pivot_clicks,\n",
    "        url=url\n",
    "    )\n",
    "    plot_click_title = \"### Visualization of click trend\"\n",
    "    display(Markdown(plot_click_title))\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    add_trend_line(ax, filtered_df_pivot_clicks_only_after_timerange, \"Date\", \"Sum of Clicks\", f\"Clicks inkl. Trend for \\\"{url}\\\"\")\n",
    "    #plt.show()\n",
    "\n",
    "    plot_avgposition_title = \"### Visualization of average ranking over time of top keyword by clicks\"\n",
    "    display(Markdown(plot_avgposition_title))\n",
    "    plot_data_inverted_y_axis(\n",
    "        title=f\"Average Ranking for \\\"{top_kw_as_string}\\\" | URL: \\\"{url}\\\"\",\n",
    "        x_label=\"Date\",\n",
    "        y_label=\"Average Ranking\",\n",
    "        data_frame=filtered_df_pivot_position,\n",
    "        url=url\n",
    "    )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
