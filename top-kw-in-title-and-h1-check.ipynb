{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "from datetime import date, timedelta\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from polyfuzz.models import TFIDF\n",
    "from polyfuzz import PolyFuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variablen anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Domain soll analysiert werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#Variables \n",
    "### Zunächst wird danach gefiltert, welche Suchbegriffe nicht enthalten sein sollen (notContains)\n",
    "brand = \"guru\"\n",
    "#AT(aut), Switzerland(che), Netherlands(nl), Spain(esp), Germany(deu)\n",
    "COUNTRY_FILTER = [\"aut\"]\n",
    "#DE(urlaubsguru.de), NLD(holidayguru.nl), ES(holidayguru.es), AT(urlaubsguru.at). CH(holidayguru.ch)\n",
    "domain_name = \"urlaubsguru.at\" \n",
    "#Hier wird der Dateipfad des ScreamingFrog Crawls angegeben - benötigt \"internal_html.csv\"\n",
    "frog_crawl_path = \"/Users/paulherzog/Downloads/internal_html_at.csv\"\n",
    "#wie viele monate an daten möchtest du haben?\n",
    "months_wanted = 3\n",
    "#Welcher Threshold soll angewandt werden? 50% heißt z.B.: Alle Title / H1 Tags, die zu weniger als 50% mit der Top Query übereinstimmen, werden als 'Quick Win' getagged\n",
    "threshold_wanted = 80\n",
    "#Wo soll das finale Excel Sheet mit den Daten gespeichert werden?\n",
    "export_path = \"/Users/paulherzog/Downloads/\"\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline GSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Bereich werden die Daten der letzten 30 Tage aus der Google Search Console gezogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variablen für den API Request\n",
    "DIMENSIONS_BYURL = [\"page\", \"query\", \"date\", \"country\"]\n",
    "DOMAIN = \"sc-domain:\" + domain_name\n",
    "credential_filepath = \".secrets/creds.json\"\n",
    "\n",
    "#berechnung der zeit\n",
    "months_in_days = int(months_wanted * 7 * 4.33)\n",
    "delta_days = months_in_days\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=delta_days)\n",
    "#variable für spätere bezeichnung der column\n",
    "months_in_days_str = str(months_in_days)\n",
    "click_column_name = \"clicks_last_\" + months_in_days_str + \"_days_for_url\"\n",
    "avg_position_column_name = \"avg_position _\" + months_in_days_str + \"_for_top_query\"\n",
    "#variable für threshold setzen\n",
    "threshold_wanted_float = threshold_wanted / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API Request GSC\n",
    "API_SERVICE_NAME = \"webmasters\"\n",
    "API_VERSION = \"v3\"\n",
    "SCOPE = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "MAX_ROWS = 25_000\n",
    "\n",
    "def auth_using_key_file(key_filepath):\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        key_filepath, scopes=SCOPE\n",
    "    )\n",
    "    service = build(API_SERVICE_NAME, API_VERSION, credentials=credentials)\n",
    "    return service\n",
    "def query(client: Resource, payload: Dict[str, str]) -> Dict[str, any]:\n",
    "    response = client.searchanalytics().query(siteUrl=DOMAIN, body=payload).execute()\n",
    "    return response\n",
    "\n",
    "KEY_FILE = credential_filepath\n",
    "service = auth_using_key_file(key_filepath=KEY_FILE)\n",
    "\n",
    "i = 0\n",
    "reponse_by_url = []\n",
    "while True:\n",
    "    payload_main_range = {\n",
    "        \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"dimensions\": DIMENSIONS_BYURL,\n",
    "        \"dimensionFilterGroups\": [{\n",
    "            \"filters\": [{\n",
    "                \"dimension\": \"country\",\n",
    "                \"operator\": \"contains\",\n",
    "                \"expression\": COUNTRY_FILTER\n",
    "                },\n",
    "            {\n",
    "                \"dimension\": \"query\",\n",
    "                \"operator\": \"notContains\",\n",
    "                \"expression\": brand\n",
    "            },\n",
    "            {\n",
    "                \"dimension\": \"page\",\n",
    "                \"operator\": \"notContains\",\n",
    "                \"expression\": \"#\"\n",
    "            }]\n",
    "        }],\n",
    "        \"rowLimit\": MAX_ROWS,\n",
    "        \"startRow\": i * MAX_ROWS\n",
    "    }\n",
    "\n",
    "    # make request to API\n",
    "    response_main_range = query(service, payload_main_range)\n",
    "\n",
    "    # if there are rows in the response, append to the temporary list\n",
    "    if response_main_range.get(\"rows\"):\n",
    "        reponse_by_url.extend(response_main_range[\"rows\"])\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    print(f\"Collected {len(reponse_by_url):,} rows.\")\n",
    "    \n",
    "# Create a DataFrame from the temporary list\n",
    "by_url_data = pd.DataFrame(reponse_by_url)\n",
    "by_url_data[DIMENSIONS_BYURL] = pd.DataFrame(by_url_data[\"keys\"].tolist(), index=by_url_data.index)\n",
    "df_raw_data = by_url_data.drop(columns=\"keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden aktueller Crawling Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Benötigt die Datei \"internal_html.csv\" aus einem aktuellen ScreamingFrog Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_crawl = pd.read_csv(frog_crawl_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufbereitung der Daten mit dem Ziel:\n",
    "- Top Keyword pro URL nach Clicks zu finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rohdaten filtern - alle queries ohne Click werden rausgehaut\n",
    "filtered_df = df_raw_data[df_raw_data[\"clicks\"] > 0]\n",
    "\n",
    "#Datum zu einem date data type umwandeln\n",
    "filtered_df[\"date\"] = pd.to_datetime(filtered_df[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "#Hier wird eine Liste aller einzigartiger URLs erstellt\n",
    "list_of_unique_urls = filtered_df.groupby(\"page\")[\"clicks\"].sum().sort_values(ascending=False).index.to_list()\n",
    "\n",
    "#dataframe mit der Summe an Klicks pro URL - wird später benötigt für einen Join\n",
    "pivot_with_performance_data = filtered_df.groupby(\"page\")[\"clicks\"].sum().sort_values(ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird durch jede einzelne dieser URLs gelooped, die Rohdaten pro URL gefiltert und anschließend das Top Keyword pro URL nach Clicks gezogen\n",
    "list_with_urls_and_top_queries = []\n",
    "\n",
    "for url in list_of_unique_urls:\n",
    "    filtered_df_for_current_url = filtered_df[filtered_df[\"page\"] == url]\n",
    "    top_kw = filtered_df_for_current_url.groupby(by=\"query\")[\"clicks\"].sum().sort_values(ascending=False).head(1).index.tolist()    \n",
    "    top_kw_str = ', '.join(top_kw)\n",
    "    list_with_urls_and_top_queries.append((url, top_kw_str))\n",
    "\n",
    "#Erstellen eines neuen Dataframes mit zwei spalten: URL und Top Query\n",
    "df_with_urls_and_top_queries = pd.DataFrame(list_with_urls_and_top_queries, columns=(\"url\", \"top_query\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier werden die Frog Daten mit den GSC Daten gejoined\n",
    "df_joined_gsc_and_frog = df_with_urls_and_top_queries.merge(frog_crawl, left_on='url', right_on='Address', how='inner')[['url', 'top_query', 'Title 1', 'H1-1']]\n",
    "# und anschließend Title und H1 auf lowercase gestellt, damit die Similarity Berechnung anschließend keine Probleme macht\n",
    "df_joined_gsc_and_frog['Title 1'] = df_joined_gsc_and_frog['Title 1'].str.lower()\n",
    "df_joined_gsc_and_frog['H1-1'] = df_joined_gsc_and_frog['H1-1'].str.lower()\n",
    "#hier wird noch der name der column umbenannt\n",
    "df_joined_gsc_and_frog = df_joined_gsc_and_frog.rename(columns={'Title 1': 'title_1'})\n",
    "df_joined_gsc_and_frog = df_joined_gsc_and_frog.rename(columns={'H1-1': 'h1'})\n",
    "#remove empty rows, where either title_1 or h1 is missing\n",
    "df_joined_gsc_and_frog = df_joined_gsc_and_frog.dropna(subset=['title_1', 'h1'])\n",
    "\n",
    "#join mit performance daten um anschließend duplicate top queries rauszuhauen, basierend auf clicks\n",
    "df_with_performance_data = pd.DataFrame(pivot_with_performance_data).reset_index()\n",
    "df_joined_gsc_and_frog_without_duplicates = df_with_performance_data.merge(df_joined_gsc_and_frog, left_on = \"page\", right_on = \"url\", how = \"inner\")\n",
    "df_joined_gsc_and_frog_without_duplicates = df_joined_gsc_and_frog_without_duplicates.sort_values(by='clicks', ascending=False).drop_duplicates(subset='top_query', keep='first')\n",
    "\n",
    "#pivot table mit Avg. Ranking pro Top KW pro URL\n",
    "list_with_top_queries_and_ranking = []\n",
    "\n",
    "for row in df_joined_gsc_and_frog_without_duplicates.itertuples():\n",
    "    url = row.page\n",
    "    top_kw_for_url = row.top_query\n",
    "    filtered_df_for_avg_pos = filtered_df[(filtered_df[\"page\"] == url) & (filtered_df[\"query\"] == top_kw_for_url)]\n",
    "    avg_pos_for_top_query = filtered_df_for_avg_pos.groupby(\"page\")[\"position\"].mean()\n",
    "    avg_pos_for_top_query = avg_pos_for_top_query[0].round(3)\n",
    "    list_with_top_queries_and_ranking.append((top_kw_for_url, avg_pos_for_top_query))\n",
    "\n",
    "#Neues Dataframe erstellen mit Top Query und Position\n",
    "df_with_top_queries_and_position = pd.DataFrame(list_with_top_queries_and_ranking, columns=(\"top_query\", avg_position_column_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Queries, Title Tags und H1 Tags werden als Listen abgespeichert\n",
    "top_query_list = df_joined_gsc_and_frog_without_duplicates[\"top_query\"].tolist()\n",
    "title_tag_list = df_joined_gsc_and_frog_without_duplicates[\"title_1\"].tolist()\n",
    "h1_list = df_joined_gsc_and_frog_without_duplicates[\"h1\"].tolist()\n",
    "\n",
    "#Hier wird PolyFuzz verwendet um die Ähnlichkeit zwischen Top Query und Title / H1 zu berechnen (Source: https://maartengr.github.io/PolyFuzz/api/models/distance/)\n",
    "model = PolyFuzz(\"EditDistance\")\n",
    "#Function\n",
    "def sim_calc(top_query_list, sim_wanted_list, column_name_for_from, column_name_for_to, column_name_for_sim):\n",
    "        #hier werden pairs erstellt zwischen top query und title/h1 - damit immer die korrekte Kombination miteinander verglichen wird\n",
    "        pairs = list(zip(top_query_list, sim_wanted_list))\n",
    "        similarities = []\n",
    "        #hier wird durch alle pairs durchgelooped und die Similarity berechnet\n",
    "        for pair in pairs:\n",
    "            model.match([pair[0]], [pair[1]])\n",
    "            similarity = model.get_matches()\n",
    "            similarity_float = similarity[\"Similarity\"][0]\n",
    "            similarities.append(similarity_float)\n",
    "\n",
    "        outcome = pd.DataFrame({\n",
    "              column_name_for_from: top_query_list,\n",
    "              column_name_for_to: sim_wanted_list,\n",
    "              column_name_for_sim: similarities\n",
    "        })\n",
    "\n",
    "        outcome.sort_values(column_name_for_sim, ascending=False, inplace=True)\n",
    "        return outcome\n",
    "\n",
    "df_top_query_title_sim = sim_calc(top_query_list, title_tag_list, \"top_query\", \"title_tag\", \"sim_topq_to_title\")\n",
    "df_top_query_h1_sim = sim_calc(top_query_list, h1_list, \"top_query\", \"h1_tag\", \"sim_topq_to_h1\")\n",
    "\n",
    "#Anschließend werden beide Tabellen miteinander gejoined\n",
    "df_joined_title_h1_similarities = df_top_query_title_sim.merge(df_top_query_h1_sim, left_on = \"top_query\", right_on = \"top_query\", how = \"inner\")\n",
    "#...und noch um die URL ergänzt\n",
    "df_with_final_similarities_and_joined_url = df_joined_title_h1_similarities.merge(df_with_urls_and_top_queries, left_on = \"top_query\", right_on = \"top_query\", how = \"inner\")\n",
    "#...und noch Ranking ergänzt\n",
    "df_with_final_similarities_and_joined_url = df_with_final_similarities_and_joined_url.merge(df_with_top_queries_and_position, on = \"top_query\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier werden jetzt noch GSC Daten hinzugezogen\n",
    "final_df_with_gsc_data = df_with_final_similarities_and_joined_url.merge(df_joined_gsc_and_frog_without_duplicates, left_on = \"url\", right_on = \"url\", how = \"inner\", suffixes=('_first', '_second'))\n",
    "final_df_with_gsc_data = final_df_with_gsc_data.rename(columns={\n",
    "    \"clicks\": click_column_name\n",
    "})\n",
    "final_df = final_df_with_gsc_data.sort_values(by=click_column_name, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird aufgeräumt - Kolumnen entfernt und jene mit Suffix umbenannt\n",
    "columns_to_drop = [col for col in final_df.columns if col.endswith('_second')]\n",
    "columns_to_rename = {col: col.replace('_first', '') for col in final_df.columns if col.endswith('_first')}\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "final_df = final_df.rename(columns=columns_to_rename)\n",
    "final_df = final_df.drop(columns=[\"page\", \"title_1\", \"h1\"])\n",
    "#anschließend kolumnen neu ordnen\n",
    "desired_columns_order = ['url', 'top_query', 'title_tag', 'sim_topq_to_title', 'h1_tag', 'sim_topq_to_h1', click_column_name, avg_position_column_name]\n",
    "final_df = final_df[desired_columns_order]\n",
    "final_df = final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalisierung & Kategorisierung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden die Daten noch getagged und unterteilt in:\n",
    "- Chancen für Title Tag Update\n",
    "    - URLs, bei denen Top Query und Title Tag weniger als x% übereinstimmt\n",
    "- Chancen für H1 Tag Update\n",
    "    - URLs, bei denen Top Query und H1 Tag weniger als x% übereinstimmt\n",
    "\n",
    "Gleichzeitig ist eine Chance nur dann eine, wenn die aktuelle Position für diese Top-Query folgende Anforderungen erfüllt:\n",
    "- Position ist schlechter/gleich wie 2 (ranking >= 2)\n",
    "\n",
    "Beide Tags sind sortiert nach Klicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird die Tagging Funktion erstellt\n",
    "def tagging(row, column_to_test):\n",
    "    value = row[column_to_test]\n",
    "    avg_position = row[avg_position_column_name]\n",
    "    if isinstance(value, (int, float)) and value <= threshold_wanted_float and avg_position >= 2:\n",
    "        return 'Chance'\n",
    "\n",
    "df = final_df\n",
    "#tagging basierend auf \n",
    "df['tag_for_title'] = df.apply(lambda row: tagging(row, 'sim_topq_to_title'), axis=1)\n",
    "df['tag_for_h1'] = df.apply(lambda row: tagging(row, 'sim_topq_to_h1'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_title_chances = df[df[\"tag_for_title\"] == \"Chance\"].drop(columns=[\"h1_tag\", \"sim_topq_to_h1\", \"tag_for_title\", \"tag_for_h1\"]).reset_index(drop=True)\n",
    "df_only_h1_chances = df[df[\"tag_for_h1\"] == \"Chance\"].drop(columns=[\"title_tag\", \"sim_topq_to_title\", \"tag_for_title\", \"tag_for_h1\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quick-Win-Übersicht für:\")\n",
    "print(f\"Domain: {domain_name}\")\n",
    "print(f\"Threshold: {threshold_wanted}%\")\n",
    "print(f\"Timerange: {start_date} - {end_date}. Dies entspricht {months_wanted} Monat(e) bzw. {months_in_days} Tage.)\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(f\"Aktuell gibt es {len(df_only_title_chances)} Chancen für einen Title-Tag-Quick-Win.\")\n",
    "print(f\"Aktuell gibt es {len(df_only_h1_chances)} Chancen für einen H1-Tag-Quick-Win.\")\n",
    "print(\"-----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_title_chances.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_h1_chances.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export der Daten als Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird der Pfad und der Dateiname gebaut\n",
    "domain_name_for_excel_name = domain_name.replace(\".\", \"_\")\n",
    "excel_file_name = export_path + end_date.strftime(\"%Y-%m-%d\") + \"-\" + domain_name_for_excel_name + \"_title_h1_quickwins.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(excel_file_name) as writer:\n",
    "    df_only_title_chances.to_excel(writer, sheet_name = \"Title Tag Quick Wins\")\n",
    "    df_only_h1_chances.to_excel(writer, sheet_name = \"H1 Tag Quick Wins\")\n",
    "    df.to_excel(writer, sheet_name=\"All data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
